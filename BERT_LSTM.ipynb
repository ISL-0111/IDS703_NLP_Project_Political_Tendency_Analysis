{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2sMmlrTBL6W"
   },
   "source": [
    "# Alejandro Paredes, Parameter tuning of BERT\n",
    "\n",
    "https://arunm8489.medium.com/understanding-distil-bert-in-depth-5f2ca92cf1ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pNpc6KmBRY4",
    "outputId": "a02a3279-1b48-48b8-da39-8402311f6642"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwAV24NJ-ZSj",
    "outputId": "ca33db64-8fcd-4613-f9d9-dba631f74faa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
      "Requirement already satisfied: contractions in /usr/local/lib/python3.10/dist-packages (0.1.73)\n",
      "Requirement already satisfied: tweet-preprocessor in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.1+cu121)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.1)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.10/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers datasets peft evaluate datasets contractions tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "C2gg1Syx-s44",
    "outputId": "eb798168-37c2-45d0-e18b-69982ec24a05"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import preprocessor as p\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yrj-hSULAi_a",
    "outputId": "c2c054b0-7912-4250-f086-488b4b093400"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 161568\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'date_publish', 'outlet', 'headline', 'lead', 'body', 'authors', 'domain', 'url', 'political_leaning'],\n",
       "        num_rows: 17952\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "df = load_dataset(\"csv\", data_files=\"/content/gdrive/MyDrive/ColabNotebooks/NLP Project/2017_2.csv\")['train'].filter(lambda example: example['headline'] is not None and example['headline'] != '').train_test_split(test_size=0.1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "_8Wqw02VBL6f"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'distilbert-base-uncased'\n",
    "\n",
    "#Define label maps\n",
    "id2label = {0:\"UNDEFINED\" ,1:\"LEFT\",2:\"RIGHT\",3:\"CENTER\"}\n",
    "label2id = {\"UNDEFINED\": 0, \"LEFT\": 1, \"RIGHT\": 2, \"CENTER\": 3}\n",
    "\n",
    "tokenizer =  DistilBertTokenizer.from_pretrained(model_checkpoint, add_prefix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "r1fchlLlBL6g"
   },
   "outputs": [],
   "source": [
    "#lemmatization and removing stopwords\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "#stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI, p.OPT.SMILEY)\n",
    "\n",
    "def preprocess(text):\n",
    "    def is_english_word(word):\n",
    "        \"\"\"Function to filter out non-English words.\"\"\"\n",
    "        return bool(re.match(r'^[a-zA-Z]+$', word))\n",
    "    text = text.lower()\n",
    "    text = contractions.fix(text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    text = p.clean(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vKd0MpK9BFPv",
    "outputId": "82415701-c8f9-4104-a2e0-91c50eee79a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  Red Cross suspends work in vast region of South Sudan \n",
      "\n",
      "Tokenized Text:  ['red', 'cross', 'suspend', '##s', 'work', 'in', 'vast', 'region', 'of', 'south', 'sudan'] \n",
      "\n",
      "Token IDs:  [2417, 2892, 28324, 2015, 2147, 1999, 6565, 2555, 1997, 2148, 10411]\n",
      "Original Text:  Corrections officer severely hurt in Kansas City jail attack \n",
      "\n",
      "Tokenized Text:  ['corrections', 'officer', 'severely', 'hurt', 'in', 'kansas', 'city', 'jail', 'attack'] \n",
      "\n",
      "Token IDs:  [20983, 2961, 8949, 3480, 1999, 5111, 2103, 7173, 2886]\n",
      "Original Text:  Sheriff: Inmate caught after 2nd jail escape within 3 weeks \n",
      "\n",
      "Tokenized Text:  ['sheriff', ':', 'inmate', 'caught', 'after', '2nd', 'jail', 'escape', 'within', '3', 'weeks'] \n",
      "\n",
      "Token IDs:  [6458, 1024, 24467, 3236, 2044, 3416, 7173, 4019, 2306, 1017, 3134]\n",
      "Original Text:  Chicago stabbing death: One donation, two suspects at large, lots of mystery \n",
      "\n",
      "Tokenized Text:  ['chicago', 'stabbing', 'death', ':', 'one', 'donation', ',', 'two', 'suspects', 'at', 'large', ',', 'lots', 'of', 'mystery'] \n",
      "\n",
      "Token IDs:  [3190, 21690, 2331, 1024, 2028, 13445, 1010, 2048, 13172, 2012, 2312, 1010, 7167, 1997, 6547]\n",
      "Original Text:  Hinkley nuclear site radioactive mud to be dumped near Cardiff \n",
      "\n",
      "Tokenized Text:  ['hi', '##nk', '##ley', 'nuclear', 'site', 'radioactive', 'mud', 'to', 'be', 'dumped', 'near', 'cardiff'] \n",
      "\n",
      "Token IDs:  [7632, 8950, 3051, 4517, 2609, 17669, 8494, 2000, 2022, 14019, 2379, 10149]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print('Original Text: ', df['train']['headline'][i], '\\n')\n",
    "    print('Tokenized Text: ', tokenizer.tokenize(preprocess(df['train']['headline'][i])), '\\n')\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['headline'][i])))\n",
    "\n",
    "#for i in range(2):\n",
    "    #print('Original Text: ', df['train']['body'][i], '\\n')\n",
    "    #print('Tokenized Text: ', tokenizer.tokenize(preprocess(df['train']['body'][i])), '\\n')\n",
    "    #print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(df['train']['body'][i])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B99-PUpHBL6i",
    "outputId": "2236512c-c887-4f9c-bb2d-3bca43abec77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "36\n",
      "0\n",
      "15\n",
      "16927\n",
      "110914\n"
     ]
    }
   ],
   "source": [
    "texts = df['train']['headline']\n",
    "\n",
    "# Handle None or missing values by filtering out None entries\n",
    "text_lengths = [len(text.split(' ')) if text is not None else 0 for text in texts]\n",
    "\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))\n",
    "\n",
    "# Count how many texts have 300 or more words\n",
    "print(sum([1 for length in text_lengths if length >= 300]))\n",
    "\n",
    "# Repeat for the 'body' column\n",
    "texts = df['train']['body']\n",
    "\n",
    "# Handle None or missing values by filtering out None entries\n",
    "text_lengths = [len(text.split()) if text is not None else 0 for text in texts]\n",
    "\n",
    "print(min(text_lengths))\n",
    "print(max(text_lengths))\n",
    "\n",
    "# Count how many texts have 300 or more words\n",
    "print(sum([1 for length in text_lengths if length >= 300]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWWgLAscBL6j"
   },
   "source": [
    "# **Creating a custom model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "k0vbHLyjBL6j"
   },
   "outputs": [],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model.\n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(model_checkpoint, num_labels=8)\n",
    "\n",
    "        # Freeze DistilBERT parameters\n",
    "        for param in self.l1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.fc1 = torch.nn.Linear(768, 1024)  # Input dimension is 768 for BERT\n",
    "        self.fc2 = torch.nn.Linear(1024, 512)\n",
    "        self.classifier = torch.nn.Linear(512, 5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        #self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        pooler = self.fc1(pooler)\n",
    "        pooler = self.relu(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        pooler = self.fc2(pooler)\n",
    "        pooler = self.relu(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        #pooler = self.fc3(pooler)\n",
    "        #pooler = self.softmax(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjBjQXj4BL6j",
    "outputId": "5e40bd38-b4d1-4dc0-e6fc-5527e3faea3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (fc1): Linear(in_features=768, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 10\n",
    "VALID_BATCH_SIZE = 10\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-04\n",
    "\n",
    "\n",
    "\n",
    "model = DistillBERTClass()\n",
    "model.to(device)\n",
    "\n",
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gUWKDy2QBL6k",
    "outputId": "0e2e9bed-41a3-4608-b03c-36896aaf047b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1.embeddings.word_embeddings.weight: requires_grad=False\n",
      "l1.embeddings.position_embeddings.weight: requires_grad=False\n",
      "l1.embeddings.LayerNorm.weight: requires_grad=False\n",
      "l1.embeddings.LayerNorm.bias: requires_grad=False\n",
      "l1.transformer.layer.0.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.0.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.0.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.0.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.0.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.0.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.0.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.0.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.0.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.0.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.0.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.0.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.0.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.0.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.0.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.0.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.1.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.1.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.1.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.1.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.1.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.1.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.1.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.1.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.1.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.1.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.1.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.1.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.1.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.1.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.1.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.1.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.2.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.2.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.2.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.2.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.2.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.2.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.2.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.2.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.2.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.2.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.2.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.2.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.2.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.2.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.2.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.2.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.3.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.3.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.3.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.3.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.3.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.3.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.3.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.3.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.3.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.3.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.3.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.3.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.3.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.3.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.3.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.3.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.4.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.4.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.4.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.4.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.4.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.4.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.4.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.4.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.4.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.4.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.4.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.4.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.4.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.4.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.4.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.4.output_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.5.attention.q_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.5.attention.q_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.5.attention.k_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.5.attention.k_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.5.attention.v_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.5.attention.v_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.5.attention.out_lin.weight: requires_grad=False\n",
      "l1.transformer.layer.5.attention.out_lin.bias: requires_grad=False\n",
      "l1.transformer.layer.5.sa_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.5.sa_layer_norm.bias: requires_grad=False\n",
      "l1.transformer.layer.5.ffn.lin1.weight: requires_grad=False\n",
      "l1.transformer.layer.5.ffn.lin1.bias: requires_grad=False\n",
      "l1.transformer.layer.5.ffn.lin2.weight: requires_grad=False\n",
      "l1.transformer.layer.5.ffn.lin2.bias: requires_grad=False\n",
      "l1.transformer.layer.5.output_layer_norm.weight: requires_grad=False\n",
      "l1.transformer.layer.5.output_layer_norm.bias: requires_grad=False\n",
      "pre_classifier.weight: requires_grad=True\n",
      "pre_classifier.bias: requires_grad=True\n",
      "fc1.weight: requires_grad=True\n",
      "fc1.bias: requires_grad=True\n",
      "fc2.weight: requires_grad=True\n",
      "fc2.bias: requires_grad=True\n",
      "classifier.weight: requires_grad=True\n",
      "classifier.bias: requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: requires_grad={param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "YmGXgdNQCb7e"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "  model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "ZgXdBTYUDLz6"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    #text = examples[\"body\"]\n",
    "    text = examples[\"headline\"]\n",
    "    labels = examples[\"political_leaning\"]\n",
    "\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,#[preprocess(t) for t in text] ,\n",
    "        return_tensors = \"np\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = 512\n",
    "        )\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = [label2id[label] for label in labels]\n",
    "    return tokenized_inputs\n",
    "\n",
    "#tokenized_dataset = df.map(tokenize_function, batched=True)\n",
    "#tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "id": "Vf28kN8fBL6l"
   },
   "outputs": [],
   "source": [
    "# Define split ratio for validation\n",
    "train_test_split = df[\"train\"].train_test_split(test_size=0.1)  # 10% for validation\n",
    "datasets = DatasetDict({\n",
    "    \"train\": train_test_split[\"train\"],\n",
    "    \"validation\": train_test_split[\"test\"],  # This is your validation set\n",
    "    \"test\": df[\"test\"],       # Keep the original test set\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "_4hEof0NBL6l"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import contractions\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define the mapping for political leaning categories to numeric values\n",
    "category_mapping = {\n",
    "    'LEFT': 0,\n",
    "    'CENTER': 1,\n",
    "    'RIGHT': 2,\n",
    "    'UNDEFINED': 3\n",
    "}\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    \"\"\" Preprocess the text to clean it for tokenization \"\"\"\n",
    "    def is_english_word(word):\n",
    "        \"\"\"Function to filter out non-English words.\"\"\"\n",
    "        return bool(re.match(r'^[a-zA-Z]+$', word))\n",
    "\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = contractions.fix(text)  # Expand contractions (e.g., \"don't\" -> \"do not\")\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = p.clean(text)  # Clean text using the clean-text library\n",
    "    return text\n",
    "\n",
    "class Triage(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.texts = dataset['headline']  # Assuming 'text' column contains the raw text\n",
    "        self.labels = dataset['political_leaning']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get raw text and label for the current index\n",
    "        text = self.texts[index]\n",
    "        label = self.labels[index]\n",
    "        tokenizer.truncation_side = \"left\"\n",
    "        #tokenized_inputs = self.tokenizer(\n",
    "        tokenized_inputs = self.tokenizer.encode_plus(\n",
    "            preprocess(text),\n",
    "            None,\n",
    "            #return_tensors=\"pt\",\n",
    "            #padding=True,\n",
    "            #truncation=True,\n",
    "            #max_length=self.max_length\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "\n",
    "        #encoding = tokenize_function({\"text\": [text], \"labels\": [label]}, self.tokenizer, self.max_length)\n",
    "        input_ids = tokenized_inputs['input_ids']  # Remove the batch dimension\n",
    "        attention_mask = tokenized_inputs['attention_mask']  # Remove the batch dimension\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'labels': torch.tensor(category_mapping[self.labels[index]], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "mvrU6xHpBL6l"
   },
   "outputs": [],
   "source": [
    "train_dataset = Triage(datasets['train'], tokenizer, max_length=512)\n",
    "val_dataset = Triage(datasets['validation'], tokenizer, max_length=512)\n",
    "test_dataset = Triage(datasets['test'], tokenizer, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "id": "On3GSFT7BL6l"
   },
   "outputs": [],
   "source": [
    "# Training DataLoader\n",
    "training_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Validation DataLoader\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "# Test DataLoader\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iR8md4S6BL6m"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "o4hduUwTGnN5"
   },
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "def calculate_accuracy(preds, targets):\n",
    "    n_correct = (preds==targets).sum().item()\n",
    "    return n_correct\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        #token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)#, token_type_ids)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calculate_accuracy(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "\n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples\n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return\n",
    "\n",
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            #token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['labels'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calculate_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "\n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return epoch_loss, epoch_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kupJM6s-BL6m",
    "outputId": "4c9b160c-c896-49ea-ef79-df30aa362096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.0981061458587646\n",
      "Training Accuracy per 5000 steps: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5002it [11:55,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.2158478236250867\n",
      "Training Accuracy per 5000 steps: 45.250949810037994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10002it [23:50,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.2069132186605769\n",
      "Training Accuracy per 5000 steps: 45.752424757524246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14542it [34:39,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 0: 46.19939344341212\n",
      "Training Loss Epoch: 1.2015696653755277\n",
      "Training Accuracy Epoch: 46.19939344341212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.2763311862945557\n",
      "Validation Accuracy per 100 steps: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1616it [03:46,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Epoch: 1.143046498815022\n",
      "Validation Accuracy Epoch: 48.542427430834934\n",
      "Saved Best Model!\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.96269690990448\n",
      "Training Accuracy per 5000 steps: 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5002it [11:54,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.1856764811702882\n",
      "Training Accuracy per 5000 steps: 47.228554289142174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10002it [23:49,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.1855292908460686\n",
      "Training Accuracy per 5000 steps: 47.24127587241276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14542it [34:39,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 1: 47.38981232506482\n",
      "Training Loss Epoch: 1.1839389795655972\n",
      "Training Accuracy Epoch: 47.38981232506482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.1015052795410156\n",
      "Validation Accuracy per 100 steps: 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1616it [03:46,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Epoch: 1.1397270067862355\n",
      "Validation Accuracy Epoch: 50.6839140929628\n",
      "Saved Best Model!\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.9601763486862183\n",
      "Training Accuracy per 5000 steps: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5002it [11:55,  7.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.1904155022977376\n",
      "Training Accuracy per 5000 steps: 47.07658468306339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10002it [23:50,  6.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.1904257130007805\n",
      "Training Accuracy per 5000 steps: 46.92530746925308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14542it [34:43,  6.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 2: 47.0528364429101\n",
      "Training Loss Epoch: 1.189491287187375\n",
      "Training Accuracy Epoch: 47.0528364429101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.0833522081375122\n",
      "Validation Accuracy per 100 steps: 70.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1616it [03:46,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Epoch: 1.155599923011395\n",
      "Validation Accuracy Epoch: 47.52738751005756\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.1276814937591553\n",
      "Training Accuracy per 5000 steps: 40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5002it [11:57,  7.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.1779514262018813\n",
      "Training Accuracy per 5000 steps: 47.58248350329934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10002it [23:54,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.179525209908342\n",
      "Training Accuracy per 5000 steps: 47.77422257774223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14542it [34:47,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 3: 47.57067897201725\n",
      "Training Loss Epoch: 1.1833471740957073\n",
      "Training Accuracy Epoch: 47.57067897201725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.2244033813476562\n",
      "Validation Accuracy per 100 steps: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1616it [03:47,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Epoch: 1.1378429900891711\n",
      "Validation Accuracy Epoch: 51.036702358111036\n",
      "Saved Best Model!\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.1576762199401855\n",
      "Training Accuracy per 5000 steps: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5002it [11:58,  7.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.1894061499608801\n",
      "Training Accuracy per 5000 steps: 47.150569886022794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10002it [23:55,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.1925137128344585\n",
      "Training Accuracy per 5000 steps: 46.923307669233076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14542it [34:48,  6.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Total Accuracy for Epoch 4: 46.95655761943732\n",
      "Training Loss Epoch: 1.1913354651826462\n",
      "Training Accuracy Epoch: 46.95655761943732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 1.1051081418991089\n",
      "Validation Accuracy per 100 steps: 50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1616it [03:46,  7.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss Epoch: 1.1976400574687684\n",
      "Validation Accuracy Epoch: 48.00396113139816\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.2208364009857178\n",
      "Training Accuracy per 5000 steps: 30.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "242it [00:34,  6.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-4aa201fd005e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-202-f18144a76ded>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnb_tr_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-2c7c3989dd44>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#tokenized_inputs = self.tokenizer(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         tokenized_inputs = self.tokenizer.encode_plus(\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m#return_tensors=\"pt\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-199-2c7c3989dd44>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to lowercase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Expand contractions (e.g., \"don't\" -> \"do not\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\x00-\\x7F]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Remove non-ASCII characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clean text using the clean-text library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    train_loss = train(epoch)\n",
    "    val_loss, val_accuracy = valid(model, val_loader)\n",
    "\n",
    "    # Save the best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"Saved Best Model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "WhWv6-PHLtfq"
   },
   "outputs": [],
   "source": [
    "!cp best_model.pt '/content/gdrive/MyDrive/ColabNotebooks/NLP Project/distilBERT/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oETAzaqeBL6m",
    "outputId": "6582bbc4-d9a8-4565-f51e-6a6d9e116941"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-206-2172e1911a5f>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (fc1): Linear(in_features=768, out_features=1024, bias=True)\n",
       "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (classifier): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxTWh0CZBL6n",
    "outputId": "4a835e32-f0a2-4322-8d33-2ea54f5d937d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Test Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|| 1796/1796 [04:12<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results\n",
      "------------------------------\n",
      "Accuracy: 0.5100\n",
      "Precision: 0.5181\n",
      "Recall: 0.5100\n",
      "F1-score: 0.4948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Test function\n",
    "def test_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Testing\"):\n",
    "            # Move batch to GPU/CPU\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    print(\"\\nTest Results\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# After training and validation, evaluate on the test set\n",
    "print(\"\\nEvaluating on Test Set\")\n",
    "test_accuracy, test_precision, test_recall, test_f1 = test_model(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "PcInvd2hBL6n",
    "outputId": "9cb360f6-bc53-4553-d6a5-dac5225290a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Test Set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-207-6967870fac08>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# After testing, plot the metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEvaluating on Test Set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Metrics and their names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plot function for metrics\n",
    "def plot_metrics(metrics, metric_names, title):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    bars = ax.bar(metric_names, metrics, color=['skyblue', 'orange', 'green', 'red'])\n",
    "\n",
    "    # Add value annotations on bars\n",
    "    for bar in bars:\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n",
    "                f\"{bar.get_height():.4f}\", ha='center', fontsize=10)\n",
    "\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_ylabel(\"Score\", fontsize=14)\n",
    "    ax.set_xlabel(\"Metrics\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# After testing, plot the metrics\n",
    "print(\"\\nEvaluating on Test Set\")\n",
    "test_accuracy, test_precision, test_recall, test_f1 = test_model(model, test_loader, device)\n",
    "\n",
    "# Metrics and their names\n",
    "metrics = [test_accuracy, test_precision, test_recall, test_f1]\n",
    "metric_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"]\n",
    "\n",
    "# Plot the test results\n",
    "plot_metrics(metrics, metric_names, title=\"Test Metrics Overview\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l84tjtX4BL6n"
   },
   "source": [
    "### Other form of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJWNUPl-BL6n"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "  predictions, labels = p\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "  return {\"accuracy\": accuracy.compute(predictions=predictions\n",
    "                                       , references=labels)}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmxsoKs4C1yh"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ORBVjXnGx19"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "lr = 1e-3\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"\"+model_checkpoint+\"lora-txt\",\n",
    "    learning_rate = lr,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    num_train_epochs = num_epochs,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end = True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_dataset[\"train\"],\n",
    "    eval_dataset = tokenized_dataset[\"test\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = data_collator,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hmS4TS65IDDV"
   },
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JuqrjJEBL6o"
   },
   "source": [
    "### Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jn1iHMyJNtM"
   },
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_dict = torch.load(\"trained_model_gral_imbd.pth\", map_location=device)\n",
    "\n",
    "text_list = ['''President-elect Trump announced on Tuesday night that he intends to appoint Linda McMahon, former CEO of World Wrestling Entertainment (WWE), to lead the Department of Education. His announcement, which was posted on Truth Social, came hours after two sources told Fox News that McMahon was likely to be picked. \"It is my great honor to announce that Linda McMahon, former Administrator of the Small Business Administration, will be the United States Secretary of Education,\" Trump's statement read.\n",
    "\"As Secretary of Education, Linda will fight tirelessly to expand Choice to every State in America, and empower parents to make the best Education decisions for their families,\" the press release added. \"Linda served for two years on the Connecticut Board of Education, where she was one of fifteen members overseeing all Public Education in the State, including its Technical High School system.\"''',\n",
    "             '''Donald Trump believes presidents have almost absolute power. In his second term, there will be few political or legal restraints to check him. The president-elects sweeping victory over Vice President Kamala Harris suddenly turned the theoretical notion that he will indulge his autocratic instincts into a genuine possibility.When Trump returns to the White House in January as one of the most powerful presidents in history, hell be able to take advantage of his own filleting of guardrails during his first presidency, which he continued through legal maneuverings out of office.''',\n",
    "             '''Nearly 100 Democrats, including Salud Carbajal, requested the Ethics Committee release its report on former Congressman Matt Gaetz's misconduct allegations. The letter, led by Rep. Sean Casten, emphasized that the Senate needs information for Gaetz's attorney general nomination. House Speaker Mike Johnson opposed releasing the report, stating Gaetz is now a \"private citizen\" and outside the panel's jurisdiction.'''\n",
    "             , ''' A South Dakota judge dismissed a lawsuit from the anti-abortion group Life Defense targeting an abortion rights measure that voters later rejected.\n",
    "Judge John Pekas dismissed the lawsuit at the request of Life Defense, which had challenged the ballot measure's petitions.\n",
    "Voters in nine states, including South Dakota, rejected abortion rights measures during the November election. '''\n",
    "             ]\n",
    "model.to('cuda')\n",
    "print('Trained model predictions')\n",
    "for text in text_list:\n",
    "  inputs = tokenizer.encode(text, return_tensors='pt').to('cuda')\n",
    "\n",
    "  logits = model(inputs).logits\n",
    "  predictions = torch.max(logits,1).indices\n",
    "\n",
    "  #print(f'{text} - {id2label[predictions.tolist()[0]]}')\n",
    "  print(f'{id2label[predictions.tolist()[0]]}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aplt_duke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
